{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-12 01:22:20 config.py:1652] Downcasting torch.float32 to torch.float16.\n",
      "INFO 10-12 01:22:20 llm_engine.py:226] Initializing an LLM engine (v0.6.1.dev238+ge2c6e0a82) with config: model='EleutherAI/gpt-j-6b', speculative_config=None, tokenizer='EleutherAI/gpt-j-6b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=EleutherAI/gpt-j-6b, use_v2_block_manager=False, num_scheduler_steps=1, multi_step_stream_outputs=False, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "INFO 10-12 01:22:23 model_runner.py:1014] Starting to load model EleutherAI/gpt-j-6b...\n",
      "INFO 10-12 01:22:24 weight_utils.py:242] Using model weights format ['*.bin']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pt checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "/opt/conda/envs/myenv/lib/python3.10/site-packages/vllm/model_executor/model_loader/weight_utils.py:424: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(bin_file, map_location=\"cpu\")\n",
      "Loading pt checkpoint shards: 100% Completed | 1/1 [00:16<00:00, 16.97s/it]\n",
      "Loading pt checkpoint shards: 100% Completed | 1/1 [00:16<00:00, 16.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-12 01:26:25 model_runner.py:1025] Loading model weights took 11.2714 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-12 01:26:25 gpu_executor.py:122] # GPU blocks: 1363, # CPU blocks: 585\n",
      "INFO 10-12 01:26:27 model_runner.py:1329] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 10-12 01:26:27 model_runner.py:1333] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 10-12 01:26:49 model_runner.py:1456] Graph capturing finished in 22 secs.\n"
     ]
    }
   ],
   "source": [
    "#llm = LLM(model=\"EleutherAI/gpt-j-6b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "prompts = [\n",
    "    \"Make a sentence using the verb 'retire'.  Sentence: \",\n",
    "    \"Make a sentence using the verb 'retire'.  Sentence: \",\n",
    "    \"Make a sentence using the verb 'retire'.  Sentence: \",\n",
    "]\n",
    "sampling_params = SamplingParams(temperature=0.8, top_p=0.95, max_tokens = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 3/3 [00:00<00:00,  5.03it/s, est. speed input: 75.41 toks/s, output: 201.08 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: \"Make a sentence using the verb 'retire'.  Sentence: \", Generated text: \"\\n\\nI like to retire in the afternoon.\\n\\nYou'll notice you've used the verb 'retire' in the sentence.  The verb 'retire' is a transitive verb\"\n",
      "Prompt: \"Make a sentence using the verb 'retire'.  Sentence: \", Generated text: '\\n\\nRetirement is ____, but it can be a pleasant change.\\n\\nA:\\n\\nThe word you are looking for is abdicate. \\nIt has the meaning of'\n",
      "Prompt: \"Make a sentence using the verb 'retire'.  Sentence: \", Generated text: '\\n\\nBy the time you retire, you will be 60 years old.\\n\\nWhat I made: \\n\\nBy the time you retire, you will be 60 years old.\\n\\nA'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "# Print the outputs.\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try out Llama-8b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 4/4 [02:26<00:00, 36.68s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.98s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(128264, 4096)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "\n",
    "MODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "PAD_TOKEN = \"<|pad|>\"\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "tokenizer.add_special_tokens({\"pad_token\": PAD_TOKEN})\n",
    "tokenizer.padding_side = \"right\"\n",
    "# takes 2-3 minutes\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=quantization_config,\n",
    "    #     attn_implementation=\"flash_attention_2\",\n",
    "    #     attn_implementation=\"sdpa\",\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model.resize_token_embeddings(len(tokenizer), pad_to_multiple_of=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=128,\n",
    "    return_full_text=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# template\n",
    "def create_test_prompt(word: str):\n",
    "    prompt_format = \"make three sentences in Japanese using the verb {}.\"\n",
    "    prompt = prompt_format.format(word)\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "make three sentences in Japanese using the verb 𠮟る.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "\n",
      "Prediction:\n",
      "Here are three sentences in Japanese using the verb 𠮟る (miru, meaning \"to see\" or \"to look\"):\n",
      "\n",
      "1. (Watashi wa ki no ji o miru.) - I look at the tree's branches.\n",
      "2. (Kare wa sono inu o miru.) - He looks at that dog.\n",
      "3. (Watashi wa asa made no sore o miru.) - I look at the sunrise until morning.\n",
      "\n",
      "Note: The verb 𠮟る (miru) can have different nuances depending on the context and the particles used. In these sentences, I used the\n",
      "\n",
      "\n",
      "\n",
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "make three sentences in Japanese using the verb 変える.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "\n",
      "Prediction:\n",
      "Here are three sentences in Japanese using the verb 変える (kaeru), which means \"to change\" or \"to exchange\":\n",
      "\n",
      "1. (Watashi wa kimono o kaeru.) - I will change my kimono.\n",
      "2. (Kono inu wa neko o kaeru.) - This dog is changing into a cat. (Note: This sentence is a bit unusual, as 変える typically implies a more literal change, but it can be used in this context to mean \"transforms into\" or \"becomes\".)\n",
      "3. (Gakkō no shuyaku o kaeru.) - I will\n",
      "\n",
      "\n",
      "\n",
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "make three sentences in Japanese using the verb 行なう.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "\n",
      "Prediction:\n",
      "Here are three sentences in Japanese using the verb 行なう (inu):\n",
      "\n",
      "1. (Watashi wa kōhī o inu) - I will go to the coffee shop.\n",
      "2. (Kare wa yūjū o inu) - He will go to the amusement park.\n",
      "3. (Watashi wa ryokō o inu) - I will go on a trip.\n",
      "\n",
      "Note: 行なう (inu) is a polite verb ending that is used to express future tense or intention. It is often used in formal or polite language.\n",
      "\n",
      "\n",
      "\n",
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "make three sentences in Japanese using the verb 冷める.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "\n",
      "Prediction:\n",
      "Here are three sentences in Japanese using the verb 冷める (sameru), which means \"to cool down\" or \"to grow cold\":\n",
      "\n",
      "1. (Kōri o sameru to, kanaeta.)\n",
      "Translation: When it gets cold, I get tired.\n",
      "2. (Watashi no sake wa hi ni sameru.)\n",
      "Translation: My sake is cooling down in the sun.\n",
      "3. (Kara no inu wa yoru ni sameru.)\n",
      "Translation: The dog's paw pads get cold at night.\n",
      "\n",
      "Note: 冷める can also be used to describe the process of something becoming less intense or severe, not\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's run some inference\n",
    "# 1. generation task\n",
    "\n",
    "# system warmup\n",
    "SYSTEM_START = \"You can speak both japanese and english fluently. Use Hiragana and kanji.\"\n",
    "USER_START = \"make three sentences in Japanese using the verb '慣れる'\"\n",
    "warmup_prompt = tokenizer.apply_chat_template(\n",
    "        [{\n",
    "            \"role\": \"system\",\n",
    "            \"content\": SYSTEM_START,\n",
    "        }, {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": USER_START,\n",
    "        }\n",
    "        ], tokenize=False, add_generation_prompt=True )\n",
    "outputs = pipe(warmup_prompt)\n",
    "\n",
    "# start\n",
    "WORDS_LIST = [\"𠮟る\",\"変える\",\"行なう\", \"冷める\"]\n",
    "prompts = [create_test_prompt(word) for word in WORDS_LIST]\n",
    "for prompt in prompts:\n",
    "    print(prompt)\n",
    "    print(\"\\nPrediction:\")\n",
    "    outputs = pipe(prompt)\n",
    "    print(outputs[0][\"generated_text\"])\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was bullshit. Let's try a model that can speak Japanese.\n",
    "sadly, there's only 70b model for llama 3 in japanese.\n",
    "### Try out Llama2-Japanese-7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/myenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.30it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "\n",
    "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "DEFAULT_SYSTEM_PROMPT = \"あなたは誠実で優秀な日本人のアシスタントです。\"\n",
    "text = \"クマが海辺に行ってアザラシと友達になり、最終的には家に帰るというプロットの短編小説を書いてください。\"\n",
    "# will take 4-6 minutes\n",
    "model_name = \"elyza/ELYZA-japanese-Llama-2-7b-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\", device_map=\"auto\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOS: <s> -> 1\n",
      "BOS: </s> -> 2\n",
      "PAD: </s> -> 2\n"
     ]
    }
   ],
   "source": [
    "print(f\"BOS: {tokenizer.bos_token} -> {tokenizer.bos_token_id}\")\n",
    "print(f\"EOS: {tokenizer.eos_token} -> {tokenizer.eos_token_id}\")\n",
    "print(f\"PAD: {tokenizer.pad_token} -> {tokenizer.pad_token_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] <<SYS>>\n",
      "あなたは誠実で優秀な日本人のアシスタントです。\n",
      "<</SYS>>\n",
      "\n",
      "[/INST] \n",
      "<s>[INST]make three different sentences in Japanese using the word '起きる'. [/INST] \n",
      "<s>[INST]make three different sentences in Japanese using the word '変える'. [/INST] \n",
      "<s>[INST]make three different sentences in Japanese using the word '行なう'. [/INST] \n",
      "<s>[INST]make three different sentences in Japanese using the word '冷める'. [/INST] \n",
      "<s>[INST]make three different sentences in Japanese using the word '謝る'. [/INST] \n",
      "<s>[INST]make three different sentences in Japanese using the word '抱く'. [/INST] \n",
      "<s>[INST]make three different sentences in Japanese using the word '決める'. [/INST] \n",
      "<s>[INST]make three different sentences in Japanese using the word '高める'. [/INST] \n"
     ]
    }
   ],
   "source": [
    "\n",
    "SYSTEM_START = \"あなたは誠実で優秀な日本人のアシスタントです。\"\n",
    "warmup_prompt = \"{bos_token}{b_inst} {system}{e_inst} \".format(\n",
    "    bos_token=tokenizer.bos_token,\n",
    "    b_inst=B_INST,\n",
    "    system=f\"{B_SYS}{SYSTEM_START}{E_SYS}\",\n",
    "    e_inst=E_INST,\n",
    ")\n",
    "print(warmup_prompt)\n",
    "\n",
    "def create_test_prompt(word: str):\n",
    "    prompt_format = \"{bos_token}{b_inst}make three different sentences in Japanese using the word '{word}'. {e_inst} \"\n",
    "    return prompt_format.format(\n",
    "        bos_token=tokenizer.bos_token,\n",
    "        b_inst=B_INST,\n",
    "        word=word,\n",
    "        e_inst=E_INST,\n",
    "    )\n",
    "\n",
    "\n",
    "WORDS_LIST = [\"起きる\",\"変える\",\"行なう\", \"冷める\", \"謝る\", \"抱く\", \"決める\", \"高める\"]\n",
    "prompts = [create_test_prompt(word) for word in WORDS_LIST]\n",
    "for prompt in prompts:\n",
    "    print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=128,\n",
    "    return_full_text=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST]make three different sentences in Japanese using the word '起きる' in various forms or tense. [/INST] \n",
      "\n",
      "Prediction:\n",
      " 承知しました。「起きる」の異なる形や時制で使われる例を3つお教えします: \n",
      "\n",
      "1. 明日、早起きしていい気分になりそうだ。\n",
      "2. 今日は早く起きるぞ！\n",
      "3. 毎朝、学校に行くのは苦痛で起きるのがつらい。\n",
      "\n",
      "いかがでしょうか\n",
      "\n",
      "\n",
      "\n",
      "<s>[INST]make three different sentences in Japanese using the word '変える' in various forms or tense. [/INST] \n",
      "\n",
      "Prediction:\n",
      " 承知しました。「変える」の異なる形や時制での使い方を3つご紹介します。\n",
      "\n",
      "1. 明日の予定を変える\n",
      "    - 明日の予定を変える (変えるの動)\n",
      "    - 明日の予定を変えよう (変えるの助)\n",
      "    - 明日の予定を変えてみよう (変えるの助)\n",
      "\n",
      "\n",
      "\n",
      "<s>[INST]make three different sentences in Japanese using the word '行なう' in various forms or tense. [/INST] \n",
      "\n",
      "Prediction:\n",
      " 承知しました。「行なう」の異なる形や時制での使い方を3つご紹介します。\n",
      "\n",
      "1. 今日は友達と一緒に映画を見る予定です。\n",
      "    (友達と一緒に映画を見る予定で今日はいます。)\n",
      "2. 明日は掃除をしないと行けないだろう。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<s>[INST]make three different sentences in Japanese using the word '冷める' in various forms or tense. [/INST] \n",
      "\n",
      "Prediction:\n",
      " 承知しました。「冷める」を用いた3つの異なる文章を日本語で作成します。\n",
      "\n",
      "1. コーヒーが冷める前に飲み切ってしまいました。\n",
      "2. 彼女の態度が冷めてしまったのは、彼のアダルトサイトの存在が原因です。\n",
      "3. この前の�\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "system_output = pipe(warmup_prompt)\n",
    "for prompt in prompts:\n",
    "    print(prompt)\n",
    "    print(\"\\nPrediction:\")\n",
    "    outputs = pipe(prompt)\n",
    "    print(outputs[0][\"generated_text\"])\n",
    "    print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.347641944885254\n"
     ]
    }
   ],
   "source": [
    "# measure inference time\n",
    "import time\n",
    "\n",
    "system_output = pipe(warmup_prompt)\n",
    "duration = 0.0\n",
    "NUM_WARMUP = 5\n",
    "NUM_REPEATS = 20\n",
    "for i in range(NUM_REPEATS+NUM_WARMUP):\n",
    "    start_time = time.time()\n",
    "    for prompt in prompts:\n",
    "        outputs = pipe(prompt)\n",
    "    if i > NUM_WARMUP:\n",
    "        duration += time.time() - start_time\n",
    "print(duration/NUM_REPEATS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.051364421844482\n"
     ]
    }
   ],
   "source": [
    "system_output = pipe(warmup_prompt)\n",
    "duration = 0.0\n",
    "for i in range(NUM_REPEATS+NUM_WARMUP):\n",
    "    start_time = time.time()\n",
    "    outputs = pipe(prompts, batch_size = len(prompts))\n",
    "    if i > NUM_WARMUP:\n",
    "        duration += time.time() - start_time\n",
    "print(duration/NUM_REPEATS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "承知しました。「行なう」を使った3つの日本語の文章は以下の通りです。\n",
      "\n",
      "1. 彼女は毎日のように洗濯物を行なう。\n",
      "2. 彼は毎週末に庭の草むしりを行なう。\n",
      "3. 私は週に3回、ジムに行き走りを行なう。\n"
     ]
    }
   ],
   "source": [
    "# let's look at the tokens more closely.\n",
    "with torch.no_grad():\n",
    "    token_ids = tokenizer.encode(prompts[2], add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "    output_ids = model.generate(\n",
    "        token_ids.to(model.device),\n",
    "        max_new_tokens=256,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "output_ids = output_ids.tolist()[0]\n",
    "# individual tokens are just one kanji/hiragana character. not vocabulary\n",
    "output = tokenizer.decode(output_ids[token_ids.size(1) :], skip_special_tokens=True)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TaggedWord(word='彼女', yomi='カノジョ', pos='代名詞'), TaggedWord(word='は', yomi='ワ', pos='助詞-係助詞'), TaggedWord(word='毎日', yomi='マイニチ', pos='名詞-普通名詞-副詞可能'), TaggedWord(word='の', yomi='ノ', pos='助詞-格助詞'), TaggedWord(word='よう', yomi='ヨー', pos='形状詞-助動詞語幹'), TaggedWord(word='に', yomi='ニ', pos='助動詞'), TaggedWord(word='洗濯', yomi='センタク', pos='名詞-普通名詞-サ変可能'), TaggedWord(word='物', yomi='ブツ', pos='接尾辞-名詞的-一般'), TaggedWord(word='を', yomi='オ', pos='助詞-格助詞'), TaggedWord(word='行なう', yomi='オコナウ', pos='動詞-一般')]\n"
     ]
    }
   ],
   "source": [
    "import MeCab\n",
    "from collections import namedtuple\n",
    "tagger = MeCab.Tagger(\"--node-format=%f%m\")\n",
    "tagged = tagger.parse(\"彼女は毎日のように濯物洗を行なう\").split('\\n')\n",
    "\n",
    "TaggedWord = namedtuple('TaggedWord', 'word, yomi, pos')\n",
    "\n",
    "parts = []\n",
    "for line in tagged:\n",
    "    tags = line.split('\\t')\n",
    "    if len(tags) >= 5:\n",
    "        parts.append(TaggedWord(word=tags[0], yomi=tags[1], pos=tags[4]))\n",
    "    \n",
    "print(parts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fast.', 'slow 3. without newline!', 'with new']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "sents = re.split(\"\\n[0-9][. ]*\", \"works out as follows\\n1.  fast.\\n2 slow 3. without newline!\\n\\n4. with new\\n\\n\")\n",
    "sents = [sent.strip() for sent in sents[1:]]\n",
    "print(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
