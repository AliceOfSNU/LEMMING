{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#llm = LLM(model=\"EleutherAI/gpt-j-6b\")\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "prompts = [\n",
    "    \"Make a sentence using the verb 'retire'.  Sentence: \",\n",
    "    \"Make a sentence using the verb 'retire'.  Sentence: \",\n",
    "    \"Make a sentence using the verb 'retire'.  Sentence: \",\n",
    "]\n",
    "sampling_params = SamplingParams(temperature=0.8, top_p=0.95, max_tokens = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "# Print the outputs.\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try out Llama-8b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "\n",
    "MODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "PAD_TOKEN = \"<|pad|>\"\n",
    "tokenizer.add_special_tokens({\"pad_token\": PAD_TOKEN})\n",
    "tokenizer.padding_side = \"right\"\n",
    "# takes 2-3 minutes\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=quantization_config,\n",
    "    #     attn_implementation=\"flash_attention_2\",\n",
    "    #     attn_implementation=\"sdpa\",\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model.resize_token_embeddings(len(tokenizer), pad_to_multiple_of=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=128,\n",
    "    return_full_text=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# template\n",
    "def create_test_prompt(word: str):\n",
    "    prompt_format = \"make three sentences in Japanese using the verb {}.\"\n",
    "    prompt = prompt_format.format(word)\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's run some inference\n",
    "# 1. generation task\n",
    "\n",
    "# system warmup\n",
    "SYSTEM_START = \"You can speak both japanese and english fluently. Use Hiragana and kanji.\"\n",
    "USER_START = \"make three sentences in Japanese using the verb '慣れる'\"\n",
    "warmup_prompt = tokenizer.apply_chat_template(\n",
    "        [{\n",
    "            \"role\": \"system\",\n",
    "            \"content\": SYSTEM_START,\n",
    "        }, {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": USER_START,\n",
    "        }\n",
    "        ], tokenize=False, add_generation_prompt=True )\n",
    "outputs = pipe(warmup_prompt)\n",
    "\n",
    "# start\n",
    "WORDS_LIST = [\"𠮟る\",\"変える\",\"行なう\", \"冷める\"]\n",
    "prompts = [create_test_prompt(word) for word in WORDS_LIST]\n",
    "for prompt in prompts:\n",
    "    print(prompt)\n",
    "    print(\"\\nPrediction:\")\n",
    "    outputs = pipe(prompt)\n",
    "    print(outputs[0][\"generated_text\"])\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was bullshit. Let's try a model that can speak Japanese.\n",
    "sadly, there's only 70b model for llama 3 in japanese.\n",
    "### Try out Llama2-Japanese-7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "\n",
    "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "DEFAULT_SYSTEM_PROMPT = \"あなたは誠実で優秀な日本人のアシスタントです。\"\n",
    "text = \"クマが海辺に行ってアザラシと友達になり、最終的には家に帰るというプロットの短編小説を書いてください。\"\n",
    "# will take 4-6 minutes\n",
    "model_name = \"elyza/ELYZA-japanese-Llama-2-7b-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\", device_map=\"auto\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=1200,\n",
    "    return_full_text=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompt.Llama2JP_prompter import Llama2JPPrompter\n",
    "\n",
    "WORDS_LIST = [\"起きる\",\"変える\",\"行なう\", \"冷める\", \"謝る\", \"抱く\", \"決める\", \"高める\"]\n",
    "\n",
    "prompter = Llama2JPPrompter(tokenizer)\n",
    "prompts = [prompter.prompt_generate_sentences_fewshot(word) for word in WORDS_LIST]\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(prompt)\n",
    "    print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pipe(prompts, batch_size = len(WORDS_LIST))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for prompt in prompts:\n",
    "    print(prompt)\n",
    "    print(\"\\nPrediction:\")\n",
    "    outputs = pipe(prompt)\n",
    "    print(outputs[0][\"generated_text\"])\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DEFAULT_SYSTEM_PROMPT = \"あなたは誠実で優秀な日本人のアシスタントです。\"\n",
    "inst = \"make three different sentences in Japanese using the word '{}'.\"\n",
    "llama_2_template = \\\n",
    "            \"{% for message in messages %}\" \\\n",
    "                \"{% if message['role'] == 'user' %}\"  \\\n",
    "                    \"{{ bos_token + '[INST] ' + message['content'].strip() + ' [/INST] ' }}\"\\\n",
    "                \"{% elif message['role'] == 'system' %}\"\\\n",
    "                    \"{{ '<<SYS>>\\\\n' + message['content'].strip() + '\\\\n<</SYS>>\\\\n\\\\n' }}\"\\\n",
    "                \"{% elif message['role'] == 'assistant' %}\"\\\n",
    "                    \"{{ ' '  + message['content'].strip() + ' ' + eos_token }}\"\\\n",
    "                \"{% endif %}\"\\\n",
    "            \"{% endfor %}\"\n",
    "tokenizer.chat_template = llama_2_template\n",
    "\n",
    "\n",
    "prompts = []\n",
    "\n",
    "for word in WORDS_LIST:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": DEFAULT_SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": inst.format(word)},\n",
    "    ]\n",
    "    prompts.append(tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    ))\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "system_output = pipe(warmup_prompt)\n",
    "for prompt in prompts:\n",
    "    print(prompt)\n",
    "    print(\"\\nPrediction:\")\n",
    "    outputs = pipe(prompt)\n",
    "    print(outputs[0][\"generated_text\"])\n",
    "    print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure inference time\n",
    "import time\n",
    "\n",
    "system_output = pipe(warmup_prompt)\n",
    "duration = 0.0\n",
    "NUM_WARMUP = 0\n",
    "NUM_REPEATS = 1\n",
    "for i in range(NUM_REPEATS+NUM_WARMUP):\n",
    "    start_time = time.time()\n",
    "    for prompt in prompts:\n",
    "        outputs = pipe(prompt)\n",
    "    finish_time = time.time()\n",
    "    if i >= NUM_WARMUP:\n",
    "        duration += finish_time - start_time\n",
    "print(len(outputs))\n",
    "print(outputs[0])\n",
    "print(duration/NUM_REPEATS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_output = pipe(warmup_prompt)\n",
    "duration = 0.0\n",
    "for i in range(NUM_REPEATS+NUM_WARMUP):\n",
    "    start_time = time.time()\n",
    "    outputs = pipe(prompts, batch_size = len(prompts))\n",
    "    finish_time = time.time()\n",
    "    if i >= NUM_WARMUP:\n",
    "        duration += finish_time - start_time\n",
    "print(len(outputs))\n",
    "print(outputs[0])\n",
    "print(duration/NUM_REPEATS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for prompt in prompts:\n",
    "    token_ids = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        generation = model.generate(\n",
    "            token_ids.to(model.device),\n",
    "            max_new_tokens=256,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            temperature=0.6,\n",
    "        )\n",
    "\n",
    "    output = tokenizer.decode(generation.tolist()[0][token_ids.size(1) :], skip_special_tokens=True)\n",
    "    print(prompt)\n",
    "    print(\"\\nPrediction:\")\n",
    "    print(output)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at the tokens more closely.\n",
    "import time\n",
    "prompt = prompts[4]\n",
    "duration = 0.0\n",
    "NUM_WARMUP = 0\n",
    "NUM_REPEATS = 1\n",
    "for i in range(NUM_REPEATS+NUM_WARMUP):\n",
    "    start_time = time.time()\n",
    "    token_ids = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        generation = model.generate(\n",
    "            token_ids.to(model.device),\n",
    "            max_new_tokens=1200,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            temperature=0.6,\n",
    "        )\n",
    "\n",
    "    output = tokenizer.decode(generation.tolist()[0][token_ids.size(1) :], skip_special_tokens=True)\n",
    "    finish_time = time.time()\n",
    "    if i >= NUM_WARMUP:\n",
    "        duration += finish_time - start_time\n",
    "\n",
    "print(prompt)\n",
    "print(\"Result:\")\n",
    "print(output)\n",
    "\n",
    "print(\"Inference Time (s)\")\n",
    "print(duration/NUM_REPEATS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's Try out Llama3-8B\n",
    "\n",
    "This is a more recent version of llama2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading shards: 100%|██████████| 4/4 [01:21<00:00, 20.25s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.68it/s]\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "仕事の熱意を取り戻すためのアイデアを5つ提案します。\n",
      "\n",
      "1. 目標の再設定: 現状の目標が曖昧や遠いものになっている可能性があります。具体的で挑戦的な目標を再設定し、達成するために必要な行動を明確にします。目標を達成するイメージを強く持つことで、仕事に対する熱意が再燃します。\n",
      "\n",
      "2. 小さな成功体験の積み重ね: 大きな目標を達成するためには、日々の小さな成功体験が必要です。小さな目標を設定し、達成することで自信を取り戻し、仕事に対する熱意が再燃します。\n",
      "\n",
      "3. 新しいスキルや知識の習得: 現状の仕事に新しいスキルや知識を加えることで、仕事に対する新鮮さや面白さを取り戻すことができます。新しいスキルや知識を習得するためには、学習や研修、セミナーなどに参加する必要があります。\n",
      "\n",
      "4. 上司や同僚とのコミュニケーション改善: 上司や同僚とのコミュニケーションがうまくいっていない場合、仕事に対する熱意が低下することがあります。コミュニケーションを改善するためには、積極的に会話をし、理解し合うことが大切です。\n",
      "\n",
      "5. 自分のやりたいことの再確認: 自分がなぜこの仕事を始めたのか、自分がやりたいことは何なのかを再確認することが大切です。仕事に対する情熱ややりがいを再確認することで、仕事に対する熱意を取り戻すことができます。\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "DEFAULT_SYSTEM_PROMPT = \"あなたは誠実で優秀な日本人のアシスタントです。特に指示が無い場合は、常に日本語で回答してください。\"\n",
    "text = \"仕事の熱意を取り戻すためのアイデアを5つ挙げてください。\"\n",
    "\n",
    "model_name = \"elyza/Llama-3-ELYZA-JP-8B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": DEFAULT_SYSTEM_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": text},\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "token_ids = tokenizer.encode(\n",
    "    prompt, add_special_tokens=False, return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        token_ids.to(model.device),\n",
    "        max_new_tokens=1200,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "output = tokenizer.decode(\n",
    "    output_ids.tolist()[0][token_ids.size(1):], skip_special_tokens=True\n",
    ")\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running this cell will make sentence task\n",
    "WORDS_LIST = [\"起きる\",\"変える\",\"行なう\", \"冷める\", \"謝る\", \"抱く\", \"決める\", \"高める\", \"泊まる\", \"伺う\", \"捨てる\", \"響く\", \"積もる\", \"読む\", \"閉じる\", \"渡す\"]\n",
    "prompts = []\n",
    "# inst = \"make three different sentences in Japanese using the word '{}'.\"\n",
    "inst = \"『{}』を使って例文を3つ作ってください。\"\n",
    "for word in WORDS_LIST:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": DEFAULT_SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": inst.format(word)},\n",
    "    ]\n",
    "    prompts.append(tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running this cell will make paragraph task prompt\n",
    "inst = \"{}について短い文を書いてください。\"\n",
    "topic = \"マイクロプラスチック\"\n",
    "prompts = []\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": DEFAULT_SYSTEM_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": inst.format(topic)},\n",
    "]\n",
    "prompts.append(tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running this cell will make paragraph task prompt\n",
    "inst = \"#命令文\\n\"\n",
    "\"以下の#入力条件と#制約条件をもとに、4択式のクイズを作成してください。出力は#出力形式に従って行ってください。#入力条件\\n\"\n",
    "\"{}\"\n",
    "\"#制約条件\\n\"\n",
    "\"- 問題数: 2問\\n\"\n",
    "\"- 問題1から問題2までのすべてについて、問題を作成する\\n\"\n",
    "\"- 回答選択肢は4択式とする\\n\"\n",
    "\"- 選択肢には数字の番号を振る\\n\"\n",
    "\"\\n\"\n",
    "\"#出力形式\\n\"\n",
    "\"- 問題1:\\n\"\n",
    "\"問題:\\n\"\n",
    "\"選択肢:\\n\"\n",
    "\"問題2:\\n\"\n",
    "\"問題:\\n\"\n",
    "\"選択肢:\\n\"\n",
    "\n",
    "topic = \"マイクロプラスチック\"\n",
    "prompts = []\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": DEFAULT_SYSTEM_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": inst.format(topic)},\n",
    "]\n",
    "prompts.append(tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "あなたは誠実で優秀な日本人のアシスタントです。特に指示が無い場合は、常に日本語で回答してください。<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "マイクロプラスチックについて短い文を書いてください。<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "\n",
      "Prediction:\n",
      "マイクロプラスチックは、5ミリメートル以下の小さなプラスチック粒子のことです。日常生活で使われるプラスチック製品が、使用や廃棄の過程で小さく砕かれて海や川に流れ込み、環境中で分解されずに残ります。小さな魚や貝などが誤って摂取し、生態系に影響を与えることが懸念されています。\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for prompt in prompts:\n",
    "    token_ids = tokenizer.encode(\n",
    "        prompt, add_special_tokens=False, return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            token_ids.to(model.device),\n",
    "            max_new_tokens=600,\n",
    "            do_sample=True,\n",
    "            temperature=0.6,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "    output = tokenizer.decode(\n",
    "        output_ids.tolist()[0][token_ids.size(1):], skip_special_tokens=True\n",
    "    )\n",
    "    print(prompt)\n",
    "    print(\"\\nPrediction:\")\n",
    "    print(output)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "あなたは誠実で優秀な日本人のアシスタントです。特に指示が無い場合は、常に日本語で回答してください。<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "『起きる』を使って例文を3つ作ってください。<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "\n",
      "Prediction:\n",
      "以下は「起きる」を使った例文です。\n",
      "\n",
      "1. 明日は早く起きるつもりなので、早く寝よう。\n",
      "2.地震で目が覚めて起きるが、外は大変なことになっていた。\n",
      "3.長い間寝ていたが、疲れが取れないので起きることにした。\n",
      "\n",
      "\n",
      "\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "あなたは誠実で優秀な日本人のアシスタントです。特に指示が無い場合は、常に日本語で回答してください。<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "『変える』を使って例文を3つ作ってください。<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "\n",
      "Prediction:\n",
      "assistant\n",
      "\n",
      "以下は、『変える』を使った例文です。\n",
      "\n",
      "1. 彼は生活習慣を変えるために、早寝早起きを心がけるようになった。\n",
      "\n",
      "2. 新しいプロジェクトが始まるにあたり、部長は社内の体制を変える決断を下した。\n",
      "\n",
      "3. この機会に、彼女は服装を変えることにした。\n",
      "\n",
      "\n",
      "\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "あなたは誠実で優秀な日本人のアシスタントです。特に指示が無い場合は、常に日本語で回答してください。<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "『行なう』を使って例文を3つ作ってください。<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "\n",
      "Prediction:\n",
      "以下は『行なう』を使用した例文です。\n",
      "\n",
      "1. 彼は新しいプロジェクトを遂行するために、チームを率いて指揮を執ることを行なう。\n",
      "\n",
      "2. 来月の社員旅行の計画を立案し、実行に移すことを行なう。\n",
      "\n",
      "3. 彼女は将来の目標を達成するために、日々の努力を継続し、計画を実行に移すことを行なう。\n",
      "\n",
      "\n",
      "\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "あなたは誠実で優秀な日本人のアシスタントです。特に指示が無い場合は、常に日本語で回答してください。<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "『冷める』を使って例文を3つ作ってください。<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "\n",
      "Prediction:\n",
      "assistant\n",
      "\n",
      "Here are three example sentences using the phrase \"冷める\":\n",
      "\n",
      "1. 彼女との恋愛は、時間が経つごとに冷めるように感じた。 (The romance felt like it was cooling down over time.)\n",
      "\n",
      "2. 冷めるのを待つ間、彼女はコーヒーを温めなおした。 (While waiting for it to cool down, she warmed up the coffee.)\n",
      "\n",
      "3. 思い出は美化されがちで、時間が経つと冷めるという。 (The memories tend to become romanticized and cool down over time.)\n",
      "\n",
      "Let me know if you need anything else!\n",
      "\n",
      "\n",
      "\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "あなたは誠実で優秀な日本人のアシスタントです。特に指示が無い場合は、常に日本語で回答してください。<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "『謝る』を使って例文を3つ作ってください。<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "\n",
      "Prediction:\n",
      "assistant\n",
      "\n",
      "Here are three example sentences using the verb \"謝る\" (あやまる):\n",
      "\n",
      "1. 彼は間違ったことをしたと謝るため、部長に会いに行った。 (He went to see the department head to apologize for his mistake.)\n",
      "2. 私は友達を傷つけたと謝ることができず、ずっと後悔していた。 (I couldn't bring myself to apologize to my friend for hurting them and regretted it for a long time.)\n",
      "3. 彼女は不倫をしていたことを謝るため、夫に別れを告げた。 (She told her husband that she was sorry for her infidelity and ended their marriage.)\n",
      "\n",
      "\n",
      "\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "あなたは誠実で優秀な日本人のアシスタントです。特に指示が無い場合は、常に日本語で回答してください。<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "『抱く』を使って例文を3つ作ってください。<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "\n",
      "Prediction:\n",
      "assistant\n",
      "\n",
      "Here are three example sentences using the verb「抱く」:\n",
      "\n",
      "1. 彼は、幼い頃に亡くなった母親の写真を抱くたびに、涙が流れた。 (He cried every time he hugged his deceased mother's photo from his childhood.)\n",
      "2. 彼女は、故郷を離れて都会で暮らすことを、寂しいと抱く心を抑えられなかった。 (She couldn't help but feel a sense of loneliness when she thought about leaving her hometown to live in the city.)\n",
      "3. 彼は、長年連れ添った妻の病気を抱く重い現実に、心が折れそうになった。 (He felt overwhelmed by the heavy reality of his wife's illness, which had been lingering for years.)\n",
      "\n",
      "\n",
      "\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "あなたは誠実で優秀な日本人のアシスタントです。特に指示が無い場合は、常に日本語で回答してください。<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "『決める』を使って例文を3つ作ってください。<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "\n",
      "Prediction:\n",
      "assistant\n",
      "\n",
      "Here are three example sentences using the verb「決める」:\n",
      "\n",
      "1. 来月の旅行先を決める会議を明日開く予定です。 (\"to decide\"の意味で使用)\n",
      "2. 彼は就職先を決めるにあたって、非常に悩んだという。 (\"to decide\"の意味で使用)\n",
      "3. 明日は天気を決める天体の動きを調べる日です。\n",
      "\n",
      "\n",
      "\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "あなたは誠実で優秀な日本人のアシスタントです。特に指示が無い場合は、常に日本語で回答してください。<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "『高める』を使って例文を3つ作ってください。<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "\n",
      "Prediction:\n",
      "assistant\n",
      "\n",
      "Here are three example sentences using the verb \"高める\" (たかめる):\n",
      "\n",
      "1. この新しい設備を導入することで、生産効率を高めることができるだろう。\n",
      "2. 彼は日々の努力を重ね、語学力を高めることに成功した。\n",
      "3. この研修プログラムは、参加者に問題解決能力を高めることを目的としている。\n",
      "\n",
      "\n",
      "\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "あなたは誠実で優秀な日本人のアシスタントです。特に指示が無い場合は、常に日本語で回答してください。<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "『泊まる』を使って例文を3つ作ってください。<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "\n",
      "Prediction:\n",
      "assistant\n",
      "\n",
      "Here are three example sentences using the verb \"泊まる\":\n",
      "\n",
      "1. 来週の土日は友達の家に泊まる予定です。 (I have plans to stay at my friend's house over the weekend.)\n",
      "2. 今夜はホテルに泊まるから、着替えを準備しておくね。 (I'll be staying at a hotel tonight, so I'll prepare my clothes.)\n",
      "3. 夏休みは実家に泊まるつもりで、久しぶりに家族と過ごす。 (I plan to stay at my hometown during the summer break and spend time with my family.)\n",
      "\n",
      "\n",
      "\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "あなたは誠実で優秀な日本人のアシスタントです。特に指示が無い場合は、常に日本語で回答してください。<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "『伺う』を使って例文を3つ作ってください。<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "\n",
      "Prediction:\n",
      "『伺う』は「うかがう」と読み、相手の所へ行って話したり、尋ねることを意味する謙譲語です。以下は例文です。\n",
      "\n",
      "1. お客様の都合を伺うために、メールで確認を取っています。 (直訳: お客様の都合を尋ねるために、メールで確認を取ります。)\n",
      "2. 明日は社長に伺う予定です。 (直訳: 明日は社長に会う予定です。)\n",
      "3. 式典の日程を伺うと、来月の15日とのことです。 (直訳: 式典の日程を尋ねると、来月の15日とのことです。)\n",
      "\n",
      "『伺う』は、相手の都合や意向を尋ねる場合や、相手の所へ行く場合に使用します。\n",
      "\n",
      "\n",
      "\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "あなたは誠実で優秀な日本人のアシスタントです。特に指示が無い場合は、常に日本語で回答してください。<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "『捨てる』を使って例文を3つ作ってください。<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "\n",
      "Prediction:\n",
      "以下は「捨てる」を使った例文です。\n",
      "\n",
      "1. 「古くなった服を捨てることにした。」\n",
      "2. 「使わない道具は捨てるか、売るかして片付ける。」\n",
      "3. 「嫌な習慣を捨てることで、生活がすっきりした。」\n",
      "\n",
      "お役に立てますか？\n",
      "\n",
      "\n",
      "\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "あなたは誠実で優秀な日本人のアシスタントです。特に指示が無い場合は、常に日本語で回答してください。<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "『響く』を使って例文を3つ作ってください。<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "\n",
      "Prediction:\n",
      "assistant\n",
      "\n",
      "Here are three example sentences using the word「響く」:\n",
      "\n",
      "1. コンサートホールで演奏されたピアノの音色は、会場全体に響く。\n",
      "(直訳: The piano's sound color resonates throughout the concert hall.)\n",
      "\n",
      "2. 彼の熱い演説は、聴衆の心に深く響く。\n",
      "(直訳: His passionate speech resonates deeply in the listeners' hearts.)\n",
      "\n",
      "3. 新しいアイデアは、社長の心に強く響く。\n",
      "(直訳: The new idea strongly resonates with the president's mind.)\n",
      "\n",
      "Let me know if you need any further assistance!\n",
      "\n",
      "\n",
      "\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "あなたは誠実で優秀な日本人のアシスタントです。特に指示が無い場合は、常に日本語で回答してください。<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "『積もる』を使って例文を3つ作ってください。<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "\n",
      "Prediction:\n",
      "以下は『積もる』を使った例文です。\n",
      "\n",
      "1. 雪が積もるにつれて、外は真っ白になった。\n",
      "（雪が降り積もることで、外が白くなった）\n",
      "\n",
      "2. 貯金が積もるように、毎月少しずつ貯めている。\n",
      "（貯金が少しずつ増えていく）\n",
      "\n",
      "3. 年月が積もるごとに、経験や知識が増えていく。\n",
      "（年を重ねるごとに、経験や知識が増えていく）\n",
      "\n",
      "\n",
      "\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "あなたは誠実で優秀な日本人のアシスタントです。特に指示が無い場合は、常に日本語で回答してください。<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "『読む』を使って例文を3つ作ってください。<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "\n",
      "Prediction:\n",
      "assistant\n",
      "\n",
      "Here are three example sentences using the word「読む」:\n",
      "\n",
      "1. この小説は一晩で読むことができた。 (This novel could be read in one night.)\n",
      "2. 新しい教科書を読むと、知識が深まる。 (Reading the new textbook deepens my knowledge.)\n",
      "3. 読むスピードが速い人は、多くの本を読むことができる。 (Fast readers can read many books.)\n",
      "\n",
      "Let me know if you'd like me to create more!\n",
      "\n",
      "\n",
      "\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "あなたは誠実で優秀な日本人のアシスタントです。特に指示が無い場合は、常に日本語で回答してください。<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "『閉じる』を使って例文を3つ作ってください。<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "\n",
      "Prediction:\n",
      "以下は『閉じる』を使った例文です。\n",
      "\n",
      "1. このファイルは、パスワードを入力しないと閉じることができない。\n",
      "\n",
      "（このファイルは、パスワードを入力しないと開くことができない）\n",
      "\n",
      "2. 電車のドアが閉じる音がしたので、急いで乗り込む必要があった。\n",
      "\n",
      "（電車のドアが閉じる音がしたので、急いで乗り込む必要があった）\n",
      "\n",
      "3. 書類の束を閉じるために、クリップを使用することが大切である。\n",
      "\n",
      "（書類の束を閉じるために、クリップを使用することが大切である）\n",
      "\n",
      "\n",
      "\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "あなたは誠実で優秀な日本人のアシスタントです。特に指示が無い場合は、常に日本語で回答してください。<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "『渡す』を使って例文を3つ作ってください。<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "\n",
      "Prediction:\n",
      "assistant\n",
      "\n",
      "Here are three example sentences using the word \"渡す\":\n",
      "\n",
      "1. 私は彼女に花束を渡すつもりです。 (I intend to give her a bouquet of flowers.)\n",
      "2. 彼は友達に旅行の切符を渡す約束をしました。 (He promised to give his friend the travel ticket.)\n",
      "3. 新入社員は上司に書類を渡すと、内容を確認してくれた。 (The new employee handed the documents to his superior, who checked their contents.)\n",
      "\n",
      "\n",
      "\n",
      "n_batch=16 \n",
      "0.34103251025080683\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# measure inference time\n",
    "import time\n",
    "\n",
    "batch_prompts = prompts[:16]\n",
    "duration = 0.0\n",
    "NUM_WARMUP = 10\n",
    "NUM_REPEATS = 20\n",
    "outputs = []\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "for i in range(NUM_REPEATS+NUM_WARMUP):\n",
    "    start_time = time.time()\n",
    "    token_ids = tokenizer(\n",
    "        batch_prompts, add_special_tokens=False, return_tensors=\"pt\", padding=True\n",
    "    ).input_ids\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            token_ids.to(model.device),\n",
    "            max_new_tokens=600,\n",
    "            do_sample=True,\n",
    "            temperature=0.6,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "    batch_outputs = []\n",
    "    for output in output_ids.tolist():\n",
    "        batch_outputs.append(tokenizer.decode(\n",
    "            output[token_ids.size(1):], skip_special_tokens=True\n",
    "        ))\n",
    "    finish_time = time.time()\n",
    "    if i >= NUM_WARMUP:\n",
    "        duration += finish_time - start_time\n",
    "    if i == 0:\n",
    "        outputs = batch_outputs\n",
    "\n",
    "for (prompt, output) in zip(batch_prompts, outputs):\n",
    "    print(prompt)\n",
    "    print(\"\\nPrediction:\")\n",
    "    print(output)\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "print(f\"n_batch={len(batch_prompts)} \")\n",
    "print(duration/len(batch_prompts)/NUM_REPEATS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantized VLLM version!\n",
    "this is an even faster version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-10-16 14:23:58,475\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "/opt/conda/lib/python3.11/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:\n",
      "No module named 'vllm._version'\n",
      "  from vllm.version import __version__ as VLLM_VERSION\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-16 14:24:02 awq_marlin.py:101] Detected that the model can run with awq_marlin, however you specified quantization=awq explicitly, so forcing awq. Use quantization=awq_marlin for faster inference\n",
      "WARNING 10-16 14:24:02 config.py:306] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 10-16 14:24:02 llm_engine.py:237] Initializing an LLM engine (vdev) with config: model='elyza/Llama-3-ELYZA-JP-8B-AWQ', speculative_config=None, tokenizer='elyza/Llama-3-ELYZA-JP-8B-AWQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=elyza/Llama-3-ELYZA-JP-8B-AWQ, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "INFO 10-16 14:24:05 model_runner.py:1060] Starting to load model elyza/Llama-3-ELYZA-JP-8B-AWQ...\n",
      "INFO 10-16 14:24:06 weight_utils.py:243] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  8.12it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.48it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.77it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-16 14:26:14 model_runner.py:1071] Loading model weights took 5.3440 GB\n",
      "INFO 10-16 14:26:16 gpu_executor.py:122] # GPU blocks: 7162, # CPU blocks: 2048\n",
      "INFO 10-16 14:26:16 gpu_executor.py:126] Maximum concurrency for 8192 tokens per request: 13.99x\n",
      "INFO 10-16 14:26:17 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 10-16 14:26:17 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 10-16 14:26:28 model_runner.py:1530] Graph capturing finished in 12 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2/2 [00:04<00:00,  2.38s/it, est. speed input: 32.79 toks/s, output: 237.93 toks/s]\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "llm = LLM(model=\"elyza/Llama-3-ELYZA-JP-8B-AWQ\", quantization=\"awq\")\n",
    "tokenizer = llm.get_tokenizer()\n",
    "\n",
    "DEFAULT_SYSTEM_PROMPT = \"あなたは誠実で優秀な日本人のアシスタントです。特に指示が無い場合は、常に日本語で回答してください。\"\n",
    "sampling_params = SamplingParams(temperature=0.6, top_p=0.9, max_tokens=600)\n",
    "messages_batch = [\n",
    "    [\n",
    "        {\"role\": \"system\", \"content\": DEFAULT_SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": \"古代ギリシャを学ぶ上で知っておくべきポイントは？\"}\n",
    "    ],\n",
    "    [\n",
    "        {\"role\": \"system\", \"content\": DEFAULT_SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": \"クマが海辺に行ってアザラシと友達になり、最終的には家に帰るというプロットの短編小説を書いてください。\"}\n",
    "    ]\n",
    "]\n",
    "\n",
    "prompts = [\n",
    "    tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    for messages in messages_batch\n",
    "]\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORDS_LIST = [\"起きる\",\"変える\",\"行なう\", \"冷める\", \"謝る\", \"抱く\", \"決める\", \"高める\", \"泊まる\", \"伺う\", \"捨てる\", \"響く\", \"積もる\", \"読む\", \"閉じる\", \"渡す\"]\n",
    "prompts = []\n",
    "# inst = \"make three different sentences in Japanese using the word '{}'.\"\n",
    "inst = \"『{}』を使って例文を3つ作ってください。\"\n",
    "for word in WORDS_LIST:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": DEFAULT_SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": inst.format(word)},\n",
    "    ]\n",
    "    prompts.append(tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.44it/s, est. speed input: 96.48 toks/s, output: 125.28 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.36it/s, est. speed input: 90.90 toks/s, output: 128.88 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.53it/s, est. speed input: 102.45 toks/s, output: 128.44 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.07it/s, est. speed input: 71.62 toks/s, output: 129.35 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.64it/s, est. speed input: 110.00 toks/s, output: 128.06 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.23s/it, est. speed input: 54.46 toks/s, output: 130.06 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.07s/it, est. speed input: 62.52 toks/s, output: 129.70 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  3.61it/s, est. speed input: 242.03 toks/s, output: 122.81 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.77it/s, est. speed input: 118.67 toks/s, output: 127.52 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.44it/s, est. speed input: 96.58 toks/s, output: 128.29 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.60it/s, est. speed input: 107.17 toks/s, output: 127.96 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s, est. speed input: 102.18 toks/s, output: 128.10 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.19it/s, est. speed input: 79.97 toks/s, output: 128.91 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.82it/s, est. speed input: 121.76 toks/s, output: 127.20 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.08s/it, est. speed input: 62.00 toks/s, output: 129.55 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  3.34it/s, est. speed input: 224.05 toks/s, output: 123.72 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.09s/it, est. speed input: 61.54 toks/s, output: 129.51 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  3.92it/s, est. speed input: 263.09 toks/s, output: 121.72 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s, est. speed input: 102.06 toks/s, output: 127.95 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  3.50it/s, est. speed input: 234.67 toks/s, output: 122.58 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.25it/s, est. speed input: 83.69 toks/s, output: 128.65 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.14s/it, est. speed input: 59.03 toks/s, output: 129.52 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.64it/s, est. speed input: 109.72 toks/s, output: 127.74 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.35it/s, est. speed input: 90.50 toks/s, output: 128.32 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s, est. speed input: 101.83 toks/s, output: 127.67 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.56it/s, est. speed input: 104.44 toks/s, output: 127.82 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s, est. speed input: 98.55 toks/s, output: 127.96 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  3.70it/s, est. speed input: 248.47 toks/s, output: 122.37 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.69it/s, est. speed input: 113.68 toks/s, output: 127.25 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  3.40it/s, est. speed input: 228.18 toks/s, output: 122.60 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "あなたは誠実で優秀な日本人のアシスタントです。特に指示が無い場合は、常に日本語で回答してください。<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "『起きる』を使って例文を3つ作ってください。<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "\n",
      "Prediction:\n",
      "以下は、『起きる』を使用した例文です。\n",
      "\n",
      "1.明日は早起きして、早朝の散歩をしてみよう。\n",
      "2.この間、突然の地震で目が覚め、起きてみると家具が倒れていた。\n",
      "3.明後日は、早起きして、遠くから見える初日の出を拝みたい。\n",
      "\n",
      "\n",
      "\n",
      "n_batch=1 \n",
      "0.6402472615242004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# measure time\n",
    "import time\n",
    "batch_prompts = prompts[:16]\n",
    "sampling_params = SamplingParams(temperature=0.6, top_p=0.9, max_tokens=600)\n",
    "duration = 0.0\n",
    "NUM_WARMUP = 10\n",
    "NUM_REPEATS = 20\n",
    "outputs = []\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "for i in range(NUM_REPEATS+NUM_WARMUP):\n",
    "    start_time = time.time()\n",
    "    vllmresults = llm.generate(batch_prompts, sampling_params)\n",
    "    finish_time = time.time()\n",
    "    if i >= NUM_WARMUP:\n",
    "        duration += finish_time - start_time\n",
    "    if i == 0:\n",
    "        outputs = [res.outputs[0].text for res in vllmresults]\n",
    "\n",
    "for (prompt, output) in zip(batch_prompts, outputs):\n",
    "    print(prompt)\n",
    "    print(\"\\nPrediction:\")\n",
    "    print(output)\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "print(f\"n_batch={len(batch_prompts)} \")\n",
    "print(duration/len(batch_prompts)/NUM_REPEATS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "from collections import namedtuple\n",
    "tagger = MeCab.Tagger(\"--node-format=%f%m\")\n",
    "tagged = tagger.parse(\"彼女は毎日のように濯物洗を行なう\").split('\\n')\n",
    "\n",
    "TaggedWord = namedtuple('TaggedWord', 'word, yomi, pos')\n",
    "\n",
    "parts = []\n",
    "for line in tagged:\n",
    "    tags = line.split('\\t')\n",
    "    if len(tags) >= 5:\n",
    "        parts.append(TaggedWord(word=tags[0], yomi=tags[1], pos=tags[4]))\n",
    "    \n",
    "print(parts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "sents = re.split(\"\\n[0-9][. ]*\", \"works out as follows\\n1.  fast.\\n2 slow 3. without newline!\\n\\n4. with new\\n\\n\")\n",
    "sents = [sent.strip() for sent in sents[1:]]\n",
    "print(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
